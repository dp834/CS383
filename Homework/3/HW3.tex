\title{CS 383 - Machine Learning}
\author{
        Assignment 3 - Dimensionality Reduction\\
}
\date{}
\documentclass[12pt]{article}
\usepackage[margin=0.7in]{geometry}
\usepackage{graphicx}
\usepackage{float}
\usepackage{comment}
\usepackage{amsmath}
\usepackage{listings}

\includecomment{versionB}
%\excludecomment{versionB}

\begin{document}
\maketitle


\section*{Introduction}
In this assignment you'll work on visualizing data, reducing its dimensionality and clustering it.\\

\noindent
You may not use any functions from machine learning library in your code, however you may use statistical functions.  For example, if available you \textbf{MAY NOT} use functions like

\begin{itemize}
\item pca
\item k-nearest neighbors functions
\end{itemize}

\noindent
Unless explicitly told to do so.  But you \textbf{MAY} use basic statistical functions like:
\begin{itemize}
\item std
\item mean
\item cov
\item eig
\end{itemize}



\section*{Grading}
\begin{table}[h]
\begin{centering}
\begin{tabular}{|l|l|}
\hline
Part 1 (Theory) & 10pts \\
Part 2 (PCA) & 40pts\\
Part 3 (Eigenfaces) & 40pts \\
Report & 10pts\\
\hline
\textbf{TOTAL} & 100pts\\
\hline
\end{tabular}
\caption{Grading Rubric}
\end{centering}
\end{table}

\newpage
\section*{DataSets}
\paragraph{Labeled Faces in the Wild Datasaet} 
This dataset consists of celebrities download from the Internet from the early 2000s.  We use the grayscale version from sklearn.datasets.\\

\noindent
we will download the images in a specific way as shown below.  You will have 3,023 images, each 87x65 pixels large, belonging to 62 different people.  
\vspace{1cm}

\begin{lstlisting}
from sklearn.datasets import fetch_lfw_people
import matplotlib.pyplot as plt
import matplotlib.cm as cm

people = fetch_lfw_people(min_faces_per_person=20, resize=0.7)
image_shape = people.images[0].shape

fig, axes = plt.subplots(2, 5, figsize=(15, 8),
                         subplot_kw={'xticks': (), 'yticks': ()})
for target, image, ax in zip(people.target, people.images, axes.ravel()):
    ax.imshow(image, cmap=cm.gray)
    ax.set_title(people.target_names[target])
\end{lstlisting} 

\vspace{1cm}
\includegraphics[width=15cm]{lfw}

\newpage
\section{Theory Questions}
\begin{enumerate}

\item Consider the following data:\\
\begin{center}
$
 \begin{bmatrix}
	-2 & 1\\
	-5 & -4\\	
	-3 & 1\\
	0 & 3\\
	-8 & 11\\
	-2 & 5\\
	1 & 0\\
	5 & -1\\
	-1 & -3\\
	6 & 1\\
\end{bmatrix}
$
\end{center}
	\begin{enumerate}
	\item Find the principle components of the data (you must show the math, including how you compute the eivenvectors and eigenvalues).  Make sure you standardize the data first and that your principle components are normalized to be unit length.  As for the amount of detail needed in your work imagine that you were working on paper with a basic calculator.  Show me whatever you would be writing on that paper.  (7pts).
	\item Project the data onto the principal component corresponding to the largest eigenvalue found in the previous part (3pts).
	\end{enumerate}
\end{enumerate}


\newpage
\section{Dimensionality Reduction via PCA}\label{pca}
Import the data as shown above.  This the labeled faces in the wild dataset.  \\
Verify that you have the correct number of people and classes \\
\vspace{1cm}
\begin{lstlisting}
print("people.images.shape: {}".format(people.images.shape))
print("Number of classes: {}".format(len(people.target_names)))

people.images.shape: (3023, 87, 65)
Number of classes: 62
\end{lstlisting} 
\vspace{1cm}
This dataset is skewed toward George W. Bush and Colin Powell as you can verify here \\
\vspace{1cm}
\begin{lstlisting}
# count how often each target appears
counts = np.bincount(people.target)
# print counts next to target names
for i, (count, name) in enumerate(zip(counts, people.target_names)):
    print("{0:25} {1:3}".format(name, count), end='   ')
    if (i + 1) % 3 == 0:
        print()
\end{lstlisting} 
\vspace{1cm}
To make the data less skewed, we will only take up to 50 images of each person (otherwise, the feature extraction would be overwhelmed by the likelihood of George W. Bush): \\
\vspace{1cm}
\begin{lstlisting}
mask = np.zeros(people.target.shape, dtype=np.bool)
for target in np.unique(people.target):
    mask[np.where(people.target == target)[0][:50]] = 1

X_people = people.data[mask]
y_people = people.target[mask]

# scale the grayscale values to be between 0 and 1
# instead of 0 and 255 for better numeric stability
X_people = X_people / 255.
\end{lstlisting} 
\vspace{1cm}

We are now going to compute how well a KNN classifier does using just the pixels alone.  
\vspace{1cm}
\begin{lstlisting}
from sklearn.neighbors import KNeighborsClassifier
# split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(
    X_people, y_people, stratify=y_people, random_state=0)
# build a KNeighborsClassifier using one neighbor
knn = KNeighborsClassifier(n_neighbors=1)
knn.fit(X_train, y_train)
print("Test set score of 1-nn: {:.2f}".format(knn.score(X_test, y_test)))
\end{lstlisting} 
\vspace{1cm}
You should have an accuracy around 23\% - 27\%. \\

\noindent
Once you have your setup complete, write a script to do the following:
\begin{enumerate}
  \item Write your own version of KNN (k=1) where you use the SSD (sum of squared differences) to compute similarity
  \item Verify that your KNN has a similar accuracy as sklearn's version
  \item Standardize your data (zero mean, divide by standard deviation)
  \item Reduces the data to 100D using PCA
  \item Compute the KNN again where K=1 with the 100D data.  Report the accuracy
  \item Compute the KNN again where K=1 with the 100D Whitened data.  Report the accuracy
   \item Reduces the data to 2D using PCA
  \item Graphs the data for visualization
\end{enumerate}

\noindent
Recall that although you may not use any package ML functions like \emph{pca}, you \textbf{may} use statistical functions like \emph{eig} or \emph{svd}.\\

\noindent
Your graph should end up looking similar to Figure \ref{PCA} (although it may be rotated differently, depending how you ordered things).
\begin{figure}[H]
\begin{center}
\includegraphics[width=9cm]{pca2d}
\caption{2D PCA Projection of data}
\label{PCA}
\end{center}
\end{figure}

\newpage
\section{Eigenfaces}\label{eigenface}
Import the data as shown above.  This the labeled faces in the wild dataset.  \\
Use the X\_train data from above.  Let's analyze the first and second principal components.\\

\noindent
\paragraph{Write a script that:}
\begin{enumerate}
  \item Imports the data as mentioned above.
  \item Standardizes the data.
  \item Performs PCA on the data (again, although you may not use any package ML functions like \emph{pca}, you \textbf{may} use statistical functions like \emph{eig}).  No need to whiten here.
  \item Find the max and min image on PC1's axis.  Find the max and min of PC2.  Plot and report the faces, what variation do these components capture?
  \item Visualizes the most important principle component as a 87x65 image (see Figure \ref{eigenface1}).
  \item Reconstructs the \emph{X\_train[0,:]} image using the primary principle component. To best see the full re-construction, ``unstandardize" the reconstruction by multiplying it by the original standard deviation and adding back in the original mean.
  \item Determines the number of principle components necessary to encode at least 95\% of the information, $k$.
 \item  Reconstructs the \emph{X\_train[0,:]} image using the $k$ most significant eigen-vectors (found in the previous step, see Figure \ref{recon3}).  For the fun of it maybe even look to see if you can perfectly reconstruct the face if you use all the eigen-vectors!  Again, to best see the full re-construction, ``unstandardize" the reconstruction by multiplying it by the original standard deviation and adding back in the original mean.
\end{enumerate}


\noindent
Your principle eigenface should end up looking similar to Figure \ref{eigenface1}.
\begin{figure}[H]
\begin{center}
\includegraphics[width=5cm]{eig1}
\caption{Primary Principle Component}
\label{eigenface1}
\end{center}
\end{figure}

Your principal reconstruction should end up looking similar to Figure \ref{recon2}.
\begin{figure}[H]
\begin{center}
\includegraphics[width=5cm]{proj1}
\caption{Reconstruction of first person}
\label{recon2}
\end{center}
\end{figure}

Your 95\% reconstruction should end up looking similar to Figure \ref{recon3}.
\begin{figure}[H]
\begin{center}
\includegraphics[width=5cm]{proj2}
\caption{Reconstruction of first person)}
\label{recon3}
\end{center}
\end{figure}


%\begin{figure}[H]
%\begin{center}
%\includegraphics[scale=0.5]{Part4_FirstClustering.png}
%\caption{Initial Clustering}
%\label{kMeans2}
%\end{center}
%\end{figure}
%
%\begin{figure}[H]
%\begin{center}
%\includegraphics[scale=0.5]{Part4_FinalClustering.png}
%\caption{Final Clustering}
%\label{kMeans3}
%\end{center}
%\end{figure}


\newpage
\section*{Submission}
For your submission, upload to Blackboard a single zip file containing:

\begin{enumerate}
\item A LaTeX typeset PDF containing:
\begin{enumerate}
\item Part 1: Your answers to the theory questions.
\item Part 2: The visualization of the PCA result, KNN accuracies
\item Part 3:
	\begin{enumerate}
	\item Visualization of primary principle component
		\item Number of principle components needed to represent 95\% of information, $k$.
	\item Visualization of the reconstruction of the first person using 
		\begin{enumerate}
		\item Original image
		\item Single principle component
		\item $k$ principle components.
		\end{enumerate}
	\end{enumerate}

\item Source Code - python notebook 
\end{enumerate}
\end{enumerate}
\end{document}

