\title{CS 383 - Machine Learning}
\author{
        Assignment 2 - Classification
}
\date{}
\documentclass[12pt]{article}
\usepackage[margin=0.7in]{geometry}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage{comment}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{tikz}

\includecomment{versionB}
%\excludecomment{versionB}

\begin{document}
\maketitle

\section{Theory}
\begin{enumerate}
\item Consider the following set of training examples for an unknown target function:  $(x_1, x_2)\rightarrow y$:
\begin{table}[h]
\begin{center}
\begin{tabular}{|l|l|l|l|}
\hline
Y & $x_1$ & $x_2$ & Count\\
\hline
+ & T & T & 3\\
+ & T & F & 4\\
+ & F & T & 4\\
+ & F & F & 1\\
- & T & T & 0\\
- & T & F & 1\\
- & F & T & 3\\
- & F & F & 5\\
\hline
\end{tabular}
\end{center}
\end{table}
	\begin{enumerate}
	\item What is the sample entropy, $H(Y)$ from this training data (using log base 2)?\\
    $H(Y) = \sum_{i=1}^n(-P(v_i)\log_nP(v_i))$
    We have 2 $v_i$ one for $+$ one for $-$.
    $$H(Y) = -\frac{12}{21}\log_2(\frac{12}{21}) + -\frac{9}{21}\log_2(\frac{9}{21}) \approx .082447$$
	\item What are the information gains for branching on variables $x_1$ and $x_2$?\\
    $$IG(A) = H(\frac{p}{p+n},\frac{n}{p+n}) - \mathbb{E}(H(A))$$
    $$H(Y) = \sum_{i=1}^n(-P(v_i)\log_nP(v_i))$$
    $$\mathbb{E}(H(A)) = \sum_{i=1}^k\frac{p_i+n_i}{p+n}H(\frac{p_i}{p_i+n_i},\frac{n_i}{p_i+n_i})$$
    Branching on $x_1$
    $$p_T = 7 \quad n_T = 1$$
    $$p_F = 5 \quad n_F = 8$$
    $$\mathbb{E}(H(x_1)) = \frac{8}{21}\cdot(-\frac{7}{21}\log_2(\frac{7}{21}) + -\frac{1}{21}\log_2(\frac{1}{21})) +
                           \frac{13}{21}\cdot(-\frac{5}{21}\log_2(\frac{5}{21}) + -\frac{8}{21}\log_2(\frac{8}{21}))
                           \approx .91445$$
    $$IG(x_1) = -\frac{12}{21}\log_2(\frac{12}{21}) + -\frac{9}{21}\log_2(\frac{9}{21}) - .91445 \approx .070778$$
    Branching on $x_2$
    $$p_T = 7 \quad n_T = 3$$
    $$p_F = 5 \quad n_F = 6$$
    $$\mathbb{E}(H(x_2)) = \frac{10}{21}\cdot(-\frac{7}{21}\log_2(\frac{7}{21}) + -\frac{3}{21}\log_2(\frac{3}{21})) +
                           \frac{11}{21}\cdot(-\frac{5}{21}\log_2(\frac{5}{21}) + -\frac{6}{21}\log_2(\frac{6}{21}))
                           \approx .971258$$
    $$IG(x_2) = -\frac{12}{21}\log_2(\frac{12}{21}) + -\frac{9}{21}\log_2(\frac{9}{21}) - .971258 \approx .0139700$$
    Since $IG(x_3) > IG(x_2)$ we branch on $x_2$
	\item Draw the deicion tree that would be learned by the ID3 algorithm without pruning from this training data?\\
	\end{enumerate}

\begin{figure}[!h]
\centering
\begin{tikzpicture}
    \tikzstyle{level 1}=[level distance = 15mm, sibling distance=35mm]
    \tikzstyle{level 2}=[level distance = 15mm, sibling distance=15mm]
    \node(0)[]{$x_1$}
        child{node(1){$x_2$}
            child{node{$+$} edge from parent node[left]{T}}
            child{node{$+$} edge from parent node[right]{F}}
            edge from parent node[left]{T}
        }
        child{node{$x_2$}
            child{node{$+$} edge from parent node[left]{T}}
            child{node{$-$} edge from parent node[right]{F}}
            edge from parent node[right]{F}
        };
\end{tikzpicture}
\end{figure}
Which can be simplified by combining the T decision on $x_1$ to always be true.
\begin{figure}[!h]
\centering
\begin{tikzpicture}
    \tikzstyle{level 1}=[level distance = 15mm, sibling distance=35mm]
    \tikzstyle{level 2}=[level distance = 15mm, sibling distance=15mm]
    \node(0)[]{$x_1$}
        child{node(1){$+$} edge from parent node[left]{T}}
        child{node{$x_2$}
            child{node{$+$} edge from parent node[left]{T}}
            child{node{$-$} edge from parent node[right]{F}}
            edge from parent node[right]{F}
        };
\end{tikzpicture}
\end{figure}
\item We decided that maybe we can use the number of characters and the average word length an essay to determine if the student should get an $A$ in a class or not.  Below are five samples of this data:
\begin{table}[h]
\begin{center}
\begin{tabular}{|l|l|l|}
\hline
\# of Chars & Average Word Length & Give an A\\
\hline
216 & 5.68 & Yes\\
69 & 4.78 & Yes\\
302 & 2.31 & No \\
60 & 3.16 & Yes \\
393 & 4.2 & No\\
\hline
\end{tabular}
\end{center}
\end{table}
	\begin{enumerate}
	\item What are the class priors, $P(A=Yes), P(A=No)$?\\
    $$P(A=Yes) = \frac{3}{5} \qquad P(A=No) = \frac{2}{5}$$

	\item Find the parameters of the Gaussians necessary to do Gaussian Naive Bayes classification on this decision to give an A or not.  Standardize the features first over all the data together so that there is no unfair bias towards the features of different scales.\\

    $$\mu_{char} = 208 \quad \sigma_{char} = 145.21536$$
    $$\mu_{len} = 4.01 \quad \sigma_{len} = 1.32348$$
    Normalized table:
\begin{table}[h]
\begin{center}
\begin{tabular}{|l|l|l|}
\hline
\# of Chars & Average Word Length & Give an A\\
\hline
\phantom{-}0.0551 & \phantom{-}1.2618 & Yes\\
-0.9572 & \phantom{-}0.5818 & Yes\\
\phantom{-}0.6473 & -1.2845 & No \\
-1.0191 & -0.6422 & Yes \\
\phantom{-}1.2740 &  \phantom{-}0.0831 & No\\
\hline
\end{tabular}
\end{center}
\end{table}\\
    Give A:
    $$\mu_{char} = -.64033 \quad \sigma_{char} = 0.60306$$
    $$\mu_{len} = .40047 \quad \sigma_{len} = 0.96487$$
    Don't give A:
    $$\mu_{char} = 0.96065 \quad \sigma_{char} = 0.44314$$
    $$\mu_{len} = -.6007 \quad \sigma_{len} = .96704$$

	\item Using your response from the prior question, determine if an essay with 242 characters and an average word length of 4.56 should get an A or not.\\
    Normalized data: chars = $0.2341$ average length = $0.41557$
    \begin{figure}[h!]
    $$P(A=Yes | [0.2341,0.41557]) = P(A=Yes)*P(c=0.2341|A=Yes)*P(l=0.41557|A=Yes)$$
    $$P(A=Yes | [0.2341,0.41557]) = \frac{3}{5}\cdot\frac{1}{0.60306\sqrt{2\pi}}e^{-\frac{(0.2341 - -0.64033)^2}{2(0.60306)^2}}\cdot\frac{1}{0.96487\sqrt{2\pi}}e^{-\frac{(0.41557 - 0.40047)^2}{2(0.96487)^2}} \approx .05735$$
    $$P(A=No | [0.2341,0.41557]) = P(A=No)*P(c=0.2341|A=No)*P(l=0.41557|A=No)$$
    $$P(A=No | [0.2341,0.41557]) = \frac{3}{5}\cdot\frac{1}{0.44314\sqrt{2\pi}}e^{-\frac{(0.2341 - 0.96065)^2}{2(0.44314)^2}}\cdot\frac{1}{0.96704\sqrt{2\pi}}e^{-\frac{(0.41557 - -0.6007)^2}{2(0.96704)^2}} \approx .03345$$
    \end{figure}\\
    Since $P(A=Yes | x) > P(A=No | x)$ we assign this paper an A.
	\end{enumerate}
\item Consider the following questions pertaining to a k-Nearest Neighbors algorithm:
	\begin{enumerate}
	\item How could you use a \emph{validation set} to determine the user-defined parameter $k$?//
    Using the validation set try different values of k and plot the recall vs precision and choose the k with the largest area under the curve.
	\end{enumerate}
\end{enumerate}


\newpage
\section{Logistic Regression}\label{naive}
In your writeup, present the thetas from gradient descent that minimize the loss function as well as plots of your method versus the built in LogisticRegression method.

My computed thetas:
\begin{tabular}{|l|l|}
\hline
Variable   & Value  \\
\hline
$\theta_0$ & 2.1681 \\
$\theta_1$ & 3.8023 \\
$\theta_2$ &-2.5665 \\
\hline
\end{tabular}

\begin{figure}[h]
\centering
\begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{images/Q2-threshold.png}
    \caption{Graph showing the boundary and likelyhood\\ of my logistic regression}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{images/Q2-cost-vs-iterations.png}
    \caption{Graph showing the loss as a function\\ of the iterations}
\end{subfigure}
\end{figure}

Built in thetas:
\begin{tabular}{|l|l|}
\hline
Variable   & Value  \\
\hline
$\theta_0$ &  49.26318 \\
$\theta_1$ &  69.73723 \\
$\theta_2$ & -30.90205 \\
\hline
\end{tabular}

\begin{figure}[h]
    \centering
    \includegraphics[scale=.5]{images/Q2-built-in-threshold.png}
    \caption{Graph showing the boundary and likelyhood of the sklearn's logistic regression}
\end{figure}
If we look at these two lines in slope intercept form we get:
$$ \text{Mine:} y = \frac{3.8023}{2.5665} + \frac{2.1681}{2.5665} \implies y = 1.4815x + 0.8448$$
$$ \text{Sklearn:} y = \frac{69.73723}{30.90205} + \frac{49.26318}{30.90205} \implies y = 2.2567x + 1.5942$$

We can see that it is close but not quite there.
I looked through sklearn's implementation and it seems to be applying some extra terms to the gradient having to do with the L1 distances.
If I increase the max iterations I see improvements getting my linear equation to $ y = 1.7646x + 1.0606$
Which correctly splits the data but seems uncertain near the boundary.

\newpage
\section{Logistic Regression Spam Classification}
Using a decision threshold of $.5$
\begin{enumerate}
    \item Precision: $0.617925$
    \item Recall:    $1.0$
    \item F-measure: $0.768484$
    \item Accuracy:  $0.682975$
\end{enumerate}
But this threshold seems to not be ideal as the precison vs recall graph shows.
\begin{figure}[h]
\centering
\includegraphics[scale =.5]{images/Q3-precision-vs-recall.png}
\end{figure}

A better threshold would be $.9$ which gives:
\begin{enumerate}
    \item Precision: $0.784566$
    \item Recall:    $0.931298$
    \item F-measure: $0.851658$
    \item Accuracy:  $0.833660$
\end{enumerate}

\newpage


\section{Naive Bayes Classifier}\label{naive}
\begin{enumerate}
    \item Precision: $0.67612$
    \item Recall:    $0.97201$
    \item F-measure: $0.79749$
    \item Accuracy:  $0.74690$
\end{enumerate}


\end{document}

